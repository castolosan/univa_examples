{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5ac9407",
   "metadata": {},
   "source": [
    "# Workshop PyTorch - Nivel B√°sico\n",
    "## Clasificaci√≥n de Neumon√≠a en Rayos X usando CNNs\n",
    "\n",
    "### Objetivo del Notebook\n",
    "En este notebook aprender√°s a implementar una red neuronal convolucional (CNN) para clasificar im√°genes de rayos X del pecho y detectar neumon√≠a. Este es un ejemplo pr√°ctico de c√≥mo PyTorch facilita el desarrollo de modelos de deep learning para aplicaciones m√©dicas.\n",
    "\n",
    "### Prerequisitos\n",
    "- Conceptos b√°sicos de PyTorch (tensores, autograd, nn.Module)\n",
    "- Conocimientos b√°sicos de redes neuronales\n",
    "- Comprensi√≥n de im√°genes digitales\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609cb82f",
   "metadata": {},
   "source": [
    "## üîß 1. Importaci√≥n de Librer√≠as\n",
    "*Tiempo estimado: 2 minutos*\n",
    "\n",
    "En esta secci√≥n importamos todas las librer√≠as necesarias para el proyecto, incluyendo PyTorch, torchvision, numpy, matplotlib, sklearn y tqdm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef7e07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos todas las librer√≠as necesarias para nuestro proyecto\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch y sus m√≥dulos principales\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Herramientas para manejo de im√°genes\n",
    "default_seed = 42\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "# M√©tricas y visualizaci√≥n\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Fijar semilla para reproducibilidad\n",
    "np.random.seed(default_seed)\n",
    "torch.manual_seed(default_seed)\n",
    "\n",
    "print(\"‚úÖ Todas las librer√≠as importadas correctamente\")\n",
    "print(f\"üî• Versi√≥n de PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaa21d8",
   "metadata": {},
   "source": [
    "## üíª 2. Configuraci√≥n del Dispositivo de C√≥mputo\n",
    "*Tiempo estimado: 1 minuto*\n",
    "\n",
    "Detectamos y configuramos el dispositivo √≥ptimo (CPU o GPU) para el entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91871ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuramos el dispositivo √≥ptimo para el entrenamiento\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Alternativa para Intel: DEVICE = \"xpu\" if torch.xpu.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"üéØ Dispositivo seleccionado: {DEVICE}\")\n",
    "print(f\"üìä Dispositivos CUDA disponibles: {torch.cuda.device_count()}\")\n",
    "\n",
    "# Informaci√≥n adicional del sistema\n",
    "if DEVICE == \"cuda\":\n",
    "    print(f\"üöÄ GPU activa: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ Memoria GPU disponible: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639f9991",
   "metadata": {},
   "source": [
    "## üìÅ 3. Descarga y Preparaci√≥n de Datos\n",
    "*Tiempo estimado: 10 minutos (dependiendo de la conexi√≥n)*\n",
    "\n",
    "Incluye instrucciones y comandos para descargar el dataset de Kaggle y preparar la estructura de carpetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f302444",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "\n",
    "# The Google Drive file ID is extracted from the shared link.\n",
    "# The shared link is: 'https://drive.google.com/file/d/1L2fXq1eabwdk10iM_eUou_W_nEdFAIjN/view?usp=sharing'\n",
    "# The file ID is '1L2fXq1eabwdk10iM_eUou_W_nEdFAIjN'\n",
    "file_id = '1L2fXq1eabwdk10iM_eUou_W_nEdFAIjN'\n",
    "output_filename = 'chest-xray-pneumonia.zip'\n",
    "\n",
    "# Use gdown to download the file by its ID\n",
    "!gdown --id {file_id} -O {output_filename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be60eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANTE: Esta celda requiere configuraci√≥n previa de Kaggle\n",
    "# Para usar esta celda, necesitas:\n",
    "# 1. Crear una cuenta en Kaggle\n",
    "# 2. Descargar tu archivo kaggle.json\n",
    "# 3. Descomenta las l√≠neas siguientes para descargar el dataset:\n",
    "\n",
    "# !pip install -q kaggle\n",
    "# !mkdir -p ~/.kaggle\n",
    "# !cp kaggle.json ~/.kaggle/\n",
    "# !chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# !kaggle datasets download -d paultimothymooney/chest-xray-pneumonia\n",
    "!unzip -q chest-xray-pneumonia.zip\n",
    "\n",
    "print(\"‚ö†Ô∏è Aseg√∫rate de tener el dataset descargado en ./chest_xray/\")\n",
    "print(\"üìã Estructura esperada:\")\n",
    "print(\"   ./chest_xray/\")\n",
    "print(\"   ‚îú‚îÄ‚îÄ train/\")\n",
    "print(\"   ‚îú‚îÄ‚îÄ val/\")\n",
    "print(\"   ‚îî‚îÄ‚îÄ test/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40073634",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è 4. Definici√≥n de Rutas y Transformaciones de Datos\n",
    "*Tiempo estimado: 5 minutos*\n",
    "\n",
    "Definimos las rutas a los conjuntos de datos y aplicamos transformaciones de aumento de datos y normalizaci√≥n para entrenamiento, validaci√≥n y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d385e574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos las rutas de nuestros datos\n",
    "data_dir = './chest_xray/'\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "val_dir = os.path.join(data_dir, 'val')\n",
    "test_dir = os.path.join(data_dir, 'test')\n",
    "\n",
    "# Configuraci√≥n de las im√°genes\n",
    "target_image_size = 224  # Tama√±o est√°ndar para muchas arquitecturas pre-entrenadas\n",
    "\n",
    "# üé® TRANSFORMACIONES PARA ENTRENAMIENTO\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((target_image_size, target_image_size)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# üéØ TRANSFORMACIONES PARA VALIDACI√ìN Y PRUEBA\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize((target_image_size, target_image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Transformaciones definidas correctamente\")\n",
    "print(f\"üìè Tama√±o de imagen: {target_image_size}x{target_image_size}\")\n",
    "print(\"üîÑ Aumento de datos activado para entrenamiento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce41348",
   "metadata": {},
   "source": [
    "## üìä 5. Carga de Datasets con ImageFolder\n",
    "*Tiempo estimado: 3 minutos*\n",
    "\n",
    "Cargamos los datasets de im√°genes usando torchvision.datasets.ImageFolder y mostramos informaci√≥n sobre las clases y el balance de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52489c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch facilita la carga de datos con ImageFolder\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=val_test_transforms)\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=val_test_transforms)\n",
    "\n",
    "# Informaci√≥n del dataset\n",
    "print(f\"üè∑Ô∏è Clases detectadas: {train_dataset.classes}\")\n",
    "print(f\"üìù Mapeo de clases: {train_dataset.class_to_idx}\")\n",
    "print(f\"üìö Im√°genes de entrenamiento: {len(train_dataset):,}\")\n",
    "print(f\"üîç Im√°genes de validaci√≥n: {len(val_dataset):,}\")\n",
    "print(f\"üß™ Im√°genes de prueba: {len(test_dataset):,}\")\n",
    "\n",
    "# Balance de clases en entrenamiento\n",
    "train_normal = len([x for x in train_dataset.imgs if x[1] == 0])\n",
    "train_pneumonia = len([x for x in train_dataset.imgs if x[1] == 1])\n",
    "print(f\"\\nüìä Balance de clases en entrenamiento:\")\n",
    "print(f\"   ü´Å Normal: {train_normal:,} ({train_normal/len(train_dataset)*100:.1f}%)\")\n",
    "print(f\"   ü¶† Neumon√≠a: {train_pneumonia:,} ({train_pneumonia/len(train_dataset)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9b4568",
   "metadata": {},
   "source": [
    "## üöÄ 6. Creaci√≥n de DataLoaders\n",
    "*Tiempo estimado: 2 minutos*\n",
    "\n",
    "Creamos DataLoaders para entrenamiento, validaci√≥n y prueba, configurando el batch size y la optimizaci√≥n para GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01824fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuramos el batch size (tama√±o de lote)\n",
    "batch_size = 32\n",
    "\n",
    "# Los DataLoaders manejan la carga eficiente de datos\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ DataLoaders creados exitosamente\")\n",
    "print(f\"üì¶ Tama√±o de batch: {batch_size}\")\n",
    "print(f\"üîÑ Batches de entrenamiento: {len(train_loader)}\")\n",
    "print(f\"üîç Batches de validaci√≥n: {len(val_loader)}\")\n",
    "print(f\"üß™ Batches de prueba: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e80da9",
   "metadata": {},
   "source": [
    "## üñºÔ∏è 7. Visualizaci√≥n de Datos\n",
    "*Tiempo estimado: 5 minutos*\n",
    "\n",
    "Mostramos ejemplos de im√°genes del dataset con sus etiquetas para verificar la correcta carga y transformaci√≥n de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edeb56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizar_imagen(tensor_imagen, titulo=None):\n",
    "    \"\"\"\n",
    "    Funci√≥n para desnormalizar y mostrar una imagen desde tensor\n",
    "    \"\"\"\n",
    "    imagen_np = tensor_imagen.numpy().transpose((1, 2, 0))\n",
    "    media = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    imagen_np = std * imagen_np + media\n",
    "    imagen_np = np.clip(imagen_np, 0, 1)\n",
    "    plt.imshow(imagen_np)\n",
    "    if titulo:\n",
    "        plt.title(titulo)\n",
    "    plt.axis('off')\n",
    "\n",
    "# Obtenemos un batch de datos de entrenamiento\n",
    "batch_imagenes, batch_etiquetas = next(iter(train_loader))\n",
    "\n",
    "# Creamos una grilla con las primeras 8 im√°genes\n",
    "fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n",
    "axes = axes.ravel()\n",
    "nombres_clases = train_dataset.classes\n",
    "for i in range(8):\n",
    "    ax = axes[i]\n",
    "    plt.sca(ax)\n",
    "    visualizar_imagen(batch_imagenes[i], f\"{nombres_clases[batch_etiquetas[i]]}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üé® Visualizaci√≥n de {len(batch_imagenes)} im√°genes del batch\")\n",
    "print(f\"üìä Forma del batch: {batch_imagenes.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e6c7ec",
   "metadata": {},
   "source": [
    "## üß† 8. Construcci√≥n de la Red Neuronal Convolucional\n",
    "*Tiempo estimado: 10 minutos*\n",
    "\n",
    "Definimos la arquitectura de la CNN personalizada para la clasificaci√≥n binaria y mostramos el resumen del modelo y el n√∫mero de par√°metros entrenables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ea5b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Neumonia(nn.Module):\n",
    "    \"\"\"\n",
    "    Red Neuronal Convolucional para clasificaci√≥n de neumon√≠a\n",
    "    Arquitectura:\n",
    "    - 3 capas convolucionales con ReLU y MaxPooling\n",
    "    - 2 capas fully connected con Dropout\n",
    "    - Salida binaria (Normal vs Neumon√≠a)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(CNN_Neumonia, self).__init__()\n",
    "        self.capas_conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.capas_fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.capas_conv(x)\n",
    "        x = self.capas_fc(x)\n",
    "        return x\n",
    "    def contar_parametros(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "modelo = CNN_Neumonia().to(DEVICE)\n",
    "print(\"üß† Modelo creado exitosamente\")\n",
    "print(f\"üìä Par√°metros entrenables: {modelo.contar_parametros():,}\")\n",
    "print(f\"üíæ Dispositivo del modelo: {next(modelo.parameters()).device}\")\n",
    "print(\"\\nüìã Arquitectura del modelo:\")\n",
    "print(modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72e9e66",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 9. Configuraci√≥n de P√©rdida y Optimizador\n",
    "*Tiempo estimado: 3 minutos*\n",
    "\n",
    "Configuramos la funci√≥n de p√©rdida BCEWithLogitsLoss, el optimizador Adam y un scheduler para el learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa3dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìâ FUNCI√ìN DE P√âRDIDA\n",
    "funcion_perdida = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# üöÄ OPTIMIZADOR\n",
    "optimizador = optim.Adam(modelo.parameters(), lr=1e-4)\n",
    "\n",
    "# üìä SCHEDULER (Opcional)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizador, step_size=3, gamma=0.1)\n",
    "\n",
    "print(\"‚öôÔ∏è Configuraci√≥n de entrenamiento:\")\n",
    "print(f\"   üéØ Funci√≥n de p√©rdida: {funcion_perdida.__class__.__name__}\")\n",
    "print(f\"   üöÄ Optimizador: {optimizador.__class__.__name__}\")\n",
    "print(f\"   üìà Learning rate inicial: {optimizador.param_groups[0]['lr']}\")\n",
    "print(f\"   üîÑ Scheduler: {scheduler.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a51bfd6",
   "metadata": {},
   "source": [
    "## üèãÔ∏è 10. Bucle de Entrenamiento Principal\n",
    "*Tiempo estimado: 15-30 minutos dependiendo del hardware*\n",
    "\n",
    "Implementamos el ciclo de entrenamiento y validaci√≥n por √©pocas, almacenando el historial de m√©tricas y mostrando el progreso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e891028",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1  # N√∫mero de √©pocas\n",
    "historial = {\n",
    "    'perdida_entrenamiento': [],\n",
    "    'perdida_validacion': [],\n",
    "    'precision_validacion': []\n",
    "}\n",
    "\n",
    "print(\"üöÄ Iniciando entrenamiento...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoca in range(EPOCHS):\n",
    "    print(f\"\\nüîÑ √âpoca {epoca + 1}/{EPOCHS}\")\n",
    "    print(\"-\" * 50)\n",
    "    # ======== FASE DE ENTRENAMIENTO ========\n",
    "    modelo.train()\n",
    "    perdida_entrenamiento = 0.0\n",
    "    barra_entrenamiento = tqdm(train_loader, desc=\"üèãÔ∏è Entrenando\", leave=False)\n",
    "    for batch_imagenes, batch_etiquetas in barra_entrenamiento:\n",
    "        batch_imagenes = batch_imagenes.to(DEVICE)\n",
    "        batch_etiquetas = batch_etiquetas.to(DEVICE).float().unsqueeze(1)\n",
    "        optimizador.zero_grad()\n",
    "        predicciones = modelo(batch_imagenes)\n",
    "        perdida = funcion_perdida(predicciones, batch_etiquetas)\n",
    "        perdida.backward()\n",
    "        optimizador.step()\n",
    "        perdida_entrenamiento += perdida.item() * batch_imagenes.size(0)\n",
    "        barra_entrenamiento.set_postfix({'P√©rdida': f\"{perdida.item():.4f}\"})\n",
    "    # ======== FASE DE VALIDACI√ìN ========\n",
    "    modelo.eval()\n",
    "    perdida_validacion = 0.0\n",
    "    predicciones_correctas = 0\n",
    "    barra_validacion = tqdm(val_loader, desc=\"üîç Validando\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch_imagenes, batch_etiquetas in barra_validacion:\n",
    "            batch_imagenes = batch_imagenes.to(DEVICE)\n",
    "            batch_etiquetas = batch_etiquetas.to(DEVICE).float().unsqueeze(1)\n",
    "            predicciones = modelo(batch_imagenes)\n",
    "            perdida = funcion_perdida(predicciones, batch_etiquetas)\n",
    "            perdida_validacion += perdida.item() * batch_imagenes.size(0)\n",
    "            predicciones_binarias = torch.sigmoid(predicciones) > 0.5\n",
    "            predicciones_correctas += torch.sum(predicciones_binarias == batch_etiquetas)\n",
    "    perdida_prom_entrenamiento = perdida_entrenamiento / len(train_dataset)\n",
    "    perdida_prom_validacion = perdida_validacion / len(val_dataset)\n",
    "    precision_validacion = predicciones_correctas.double() / len(val_dataset)\n",
    "    historial['perdida_entrenamiento'].append(perdida_prom_entrenamiento)\n",
    "    historial['perdida_validacion'].append(perdida_prom_validacion)\n",
    "    historial['precision_validacion'].append(precision_validacion.item())\n",
    "    scheduler.step()\n",
    "    print(f\"üìä Resultados de la √©poca {epoca + 1}:\")\n",
    "    print(f\"   üèãÔ∏è P√©rdida entrenamiento: {perdida_prom_entrenamiento:.4f}\")\n",
    "    print(f\"   üîç P√©rdida validaci√≥n: {perdida_prom_validacion:.4f}\")\n",
    "    print(f\"   üéØ Precisi√≥n validaci√≥n: {precision_validacion:.4f} ({precision_validacion*100:.1f}%)\")\n",
    "    print(f\"   üìà Learning rate: {optimizador.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "print(\"\\nüéâ ¬°Entrenamiento completado exitosamente!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2e4299",
   "metadata": {},
   "source": [
    "## üìà 11. Visualizaci√≥n de Curvas de Aprendizaje\n",
    "*Tiempo estimado: 5 minutos*\n",
    "\n",
    "Graficamos la evoluci√≥n de la p√©rdida y precisi√≥n durante el entrenamiento y validaci√≥n, e identificamos posibles signos de overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54b64cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "# Gr√°fico 1: P√©rdida vs √âpocas\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(range(1, EPOCHS + 1), historial['perdida_entrenamiento'], 'b-o', label='Entrenamiento', linewidth=2, markersize=6)\n",
    "plt.plot(range(1, EPOCHS + 1), historial['perdida_validacion'], 'r-o', label='Validaci√≥n', linewidth=2, markersize=6)\n",
    "plt.title('üìâ Evoluci√≥n de la P√©rdida', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('√âpocas')\n",
    "plt.ylabel('P√©rdida')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "# Gr√°fico 2: Precisi√≥n vs √âpocas\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(range(1, EPOCHS + 1), historial['precision_validacion'], 'g-o', label='Validaci√≥n', linewidth=2, markersize=6)\n",
    "plt.title('üéØ Evoluci√≥n de la Precisi√≥n', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('√âpocas')\n",
    "plt.ylabel('Precisi√≥n')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "# Gr√°fico 3: Comparaci√≥n P√©rdida Entrenamiento vs Validaci√≥n\n",
    "plt.subplot(1, 3, 3)\n",
    "diferencia = np.array(historial['perdida_validacion']) - np.array(historial['perdida_entrenamiento'])\n",
    "plt.plot(range(1, EPOCHS + 1), diferencia, 'purple', linewidth=2, marker='o')\n",
    "plt.title('üîç Diferencia de P√©rdida\\n(Validaci√≥n - Entrenamiento)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('√âpocas')\n",
    "plt.ylabel('Diferencia')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# An√°lisis autom√°tico de las curvas\n",
    "print(\"üîç An√°lisis de las curvas de aprendizaje:\")\n",
    "print(f\"   üìä Precisi√≥n final: {historial['precision_validacion'][-1]:.4f}\")\n",
    "print(f\"   üìà Mejora en precisi√≥n: {historial['precision_validacion'][-1] - historial['precision_validacion'][0]:.4f}\")\n",
    "if historial['perdida_validacion'][-1] > historial['perdida_entrenamiento'][-1]:\n",
    "    print(\"   ‚ö†Ô∏è  Posible overfitting detectado\")\n",
    "else:\n",
    "    print(\"   ‚úÖ No hay signos claros de overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caea1e35",
   "metadata": {},
   "source": [
    "## üé≠ 12. Matriz de Confusi√≥n y Evaluaci√≥n Final\n",
    "*Tiempo estimado: 5 minutos*\n",
    "\n",
    "Evaluamos el modelo en el conjunto de prueba, generamos la matriz de confusi√≥n y calculamos m√©tricas detalladas como precisi√≥n, sensibilidad y especificidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747929b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß™ Evaluando modelo en conjunto de prueba...\")\n",
    "todas_predicciones = []\n",
    "todas_etiquetas = []\n",
    "probabilidades = []\n",
    "modelo.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_imagenes, batch_etiquetas in tqdm(test_loader, desc=\"Evaluando\"):\n",
    "        batch_imagenes = batch_imagenes.to(DEVICE)\n",
    "        batch_etiquetas = batch_etiquetas.to(DEVICE)\n",
    "        logits = modelo(batch_imagenes)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        predicciones = probs > 0.5\n",
    "        todas_predicciones.extend(predicciones.cpu().numpy().flatten())\n",
    "        todas_etiquetas.extend(batch_etiquetas.cpu().numpy())\n",
    "        probabilidades.extend(probs.cpu().numpy().flatten())\n",
    "todas_predicciones = np.array(todas_predicciones, dtype=int)\n",
    "todas_etiquetas = np.array(todas_etiquetas)\n",
    "probabilidades = np.array(probabilidades)\n",
    "cm = confusion_matrix(todas_etiquetas, todas_predicciones)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=test_dataset.classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "plt.title('üé≠ Matriz de Confusi√≥n - Conjunto de Prueba', fontsize=16, fontweight='bold')\n",
    "plt.show()\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print(\"\\nüìä Reporte de Clasificaci√≥n:\")\n",
    "print(classification_report(todas_etiquetas, todas_predicciones, target_names=test_dataset.classes))\n",
    "accuracy = accuracy_score(todas_etiquetas, todas_predicciones)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "sensitivity = tp / (tp + fn)  # Sensibilidad (Recall)\n",
    "specificity = tn / (tn + fp)  # Especificidad\n",
    "precision = tp / (tp + fp)    # Precisi√≥n\n",
    "print(f\"\\nüéØ M√©tricas Importantes:\")\n",
    "print(f\"   üìà Precisi√≥n General: {accuracy:.4f} ({accuracy*100:.1f}%)\")\n",
    "print(f\"   üîç Sensibilidad (Recall): {sensitivity:.4f} ({sensitivity*100:.1f}%)\")\n",
    "print(f\"   ‚úÖ Especificidad: {specificity:.4f} ({specificity*100:.1f}%)\")\n",
    "print(f\"   üéØ Precisi√≥n (Pneumonia): {precision:.4f} ({precision*100:.1f}%)\")\n",
    "print(f\"\\nüî¢ Matriz de Confusi√≥n:\")\n",
    "print(f\"   ‚úÖ Verdaderos Negativos (TN): {tn}\")\n",
    "print(f\"   ‚ùå Falsos Positivos (FP): {fp}\")\n",
    "print(f\"   ‚ùå Falsos Negativos (FN): {fn}\")\n",
    "print(f\"   ‚úÖ Verdaderos Positivos (TP): {tp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1faaeda",
   "metadata": {},
   "source": [
    "## ü§î 13. Reflexi√≥n √âtica y Discusi√≥n\n",
    "*Tiempo estimado: 10 minutos*\n",
    "\n",
    "Analizamos los errores del modelo desde una perspectiva m√©dica y √©tica, calculamos el AUC-ROC y visualizamos la curva ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a540f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üè• AN√ÅLISIS √âTICO Y M√âDICO\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"‚ùå Falsos Positivos (FP = {fp}):\")\n",
    "print(\"   ‚Ä¢ Diagnosticar neumon√≠a a una persona sana\")\n",
    "print(\"   ‚Ä¢ Consecuencias: Estr√©s, pruebas adicionales, costos\")\n",
    "print(\"   ‚Ä¢ Gravedad: MEDIA\")\n",
    "print(f\"\\n‚ùå Falsos Negativos (FN = {fn}):\")\n",
    "print(\"   ‚Ä¢ NO diagnosticar neumon√≠a a una persona enferma\")\n",
    "print(\"   ‚Ä¢ Consecuencias: Progresi√≥n de la enfermedad, riesgo de vida\")\n",
    "print(\"   ‚Ä¢ Gravedad: ALTA\")\n",
    "print(f\"\\nüéØ En medicina, generalmente preferimos:\")\n",
    "print(\"   ‚Ä¢ ALTA sensibilidad (pocos falsos negativos)\")\n",
    "print(\"   ‚Ä¢ Aceptar m√°s falsos positivos que falsos negativos\")\n",
    "print(\"   ‚Ä¢ Principio: 'Mejor prevenir que lamentar'\")\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, thresholds = roc_curve(todas_etiquetas, probabilidades)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "print(f\"\\nüìä An√°lisis ROC:\")\n",
    "print(f\"   üéØ AUC-ROC: {roc_auc:.4f}\")\n",
    "print(f\"   ‚öñÔ∏è Punto de corte √≥ptimo: {optimal_threshold:.4f}\")\n",
    "print(f\"   üìà Sensibilidad √≥ptima: {tpr[optimal_idx]:.4f}\")\n",
    "print(f\"   üìâ Especificidad √≥ptima: {1-fpr[optimal_idx]:.4f}\")\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f}')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('Tasa de Falsos Positivos (1 - Especificidad)')\n",
    "plt.ylabel('Tasa de Verdaderos Positivos (Sensibilidad)')\n",
    "plt.title('Curva ROC - Clasificaci√≥n de Neumon√≠a')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLRM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
